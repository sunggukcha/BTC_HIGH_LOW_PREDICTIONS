{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a30df4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import datetime\n",
    "import json\n",
    "import os\n",
    "opj = os.path.join\n",
    "\n",
    "import ccxt\n",
    "import numpy\n",
    "import pandas as pd\n",
    "import pandas_ta as ta # needed for processing chart data \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from transformers import AlbertTokenizer, AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "\n",
    "from tokenization_roberta_spm import FairSeqRobertaSentencePieceTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe81beb",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2a65060",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_name = \"8hours\"\n",
    "_hours = 8\n",
    "\n",
    "device = torch.device('cuda')\n",
    "\n",
    "configs = {}\n",
    "def classifier_generator(config):\n",
    "    tokenizer = config.tokenizer.from_pretrained(config.tokenizer_name)\n",
    "    model = config.model.from_pretrained(config.model_name)\n",
    "    model.to(device)\n",
    "    \n",
    "    def classifier(title, content):\n",
    "        encoded_inputs = tokenizer(title, content, return_tensors=\"pt\", max_length=512, truncation=True).to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(**encoded_inputs)\n",
    "            logits = output['logits']\n",
    "        res = nn.Softmax(dim=1)(logits)[0]\n",
    "        return res\n",
    "    \n",
    "    def free_model():\n",
    "        model.to('cpu')\n",
    "    \n",
    "    return classifier, free_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e4507ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self, tokenizer, tokenizer_name, model, model_name):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tokenizer_name = tokenizer_name\n",
    "        self.model = model\n",
    "        self.model_name = model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e39e562a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### deberta\n",
    "tokenizer = AutoTokenizer\n",
    "tokenizer_name = \"totoro4007/cryptodeberta-base-all-finetuned\"\n",
    "model = AutoModelForSequenceClassification\n",
    "model_name = \"totoro4007/cryptodeberta-base-all-finetuned\"\n",
    "deberta_config = Config(\n",
    "    tokenizer = tokenizer,\n",
    "    tokenizer_name = tokenizer_name,\n",
    "    model = model,\n",
    "    model_name = model_name\n",
    ")\n",
    "configs['deberta'] = deberta_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9ec4623",
   "metadata": {},
   "outputs": [],
   "source": [
    "### roberta\n",
    "tokenizer = FairSeqRobertaSentencePieceTokenizer\n",
    "tokenizer_name = \"fairseq-roberta-all-model\"\n",
    "model = AutoModelForSequenceClassification\n",
    "model_name = \"totoro4007/cryptoroberta-base-all-finetuned\"\n",
    "roberta_config = Config(\n",
    "    tokenizer = tokenizer,\n",
    "    tokenizer_name = tokenizer_name,\n",
    "    model = model,\n",
    "    model_name = model_name\n",
    ")\n",
    "configs['roberta'] = roberta_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd5f2deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### bert\n",
    "tokenizer = AlbertTokenizer\n",
    "tokenizer_name = \"totoro4007/cryptobert-base-all-finetuned\"\n",
    "model = AutoModelForSequenceClassification\n",
    "model_name = \"totoro4007/cryptobert-base-all-finetuned\"\n",
    "bert_config = Config(\n",
    "    tokenizer = tokenizer,\n",
    "    tokenizer_name = tokenizer_name,\n",
    "    model = model,\n",
    "    model_name = model_name\n",
    ")\n",
    "configs['bert'] = bert_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b24c80b",
   "metadata": {},
   "source": [
    "## Generate df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f722c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def res2vec(res):\n",
    "    vec = [0, 0, 0]\n",
    "    for r in res:\n",
    "        label = int(r['label'].split('_')[-1])\n",
    "        score = round(r['score'], 4)\n",
    "        vec[label] = score\n",
    "    return vec\n",
    "\n",
    "def vec_sum(vec1, vec2):\n",
    "    return [vec1[0]+vec2[0], vec1[1]+vec2[1], vec1[2]+vec2[2]]\n",
    "\n",
    "def process(df):\n",
    "    binance = ccxt.binance() \n",
    "    dates = df['timestamp'].values \n",
    "    timestamp = [] \n",
    "    for i in range(len(dates)):\n",
    "        date_string = binance.iso8601(int(dates[i])) \n",
    "        date_string = date_string[:10] + \" \" + date_string[11:-5] \n",
    "        timestamp.append(date_string) \n",
    "    df['datetime'] = timestamp \n",
    "    df = df.drop(columns={'timestamp'}) \n",
    "    return df\n",
    "\n",
    "def dates2datetimes(dates):\n",
    "    res = []\n",
    "    for date in dates:\n",
    "        date = list(map(int, date.split('-')))\n",
    "        neo = datetime.datetime(date[0], date[1], date[2], date[3])\n",
    "        res.append(neo)\n",
    "    return res\n",
    "\n",
    "def closest(query, dates):\n",
    "    dates = dates2datetimes(dates)\n",
    "    st = 0\n",
    "    en = len(dates)-1\n",
    "    while True:\n",
    "        mid = (st + en) // 2\n",
    "        if dates[mid] < query:\n",
    "            st = mid\n",
    "        else:\n",
    "            en = mid\n",
    "        if mid == (st + en) // 2:\n",
    "            break            \n",
    "    return mid\n",
    "\n",
    "def isin(query, date):\n",
    "    date = list(map(int, date.split('-')))\n",
    "    date = datetime.datetime(date[0], date[1], date[2], date[3])\n",
    "    hour = datetime.timedelta(hours=1)\n",
    "    delta = query - date\n",
    "    delta = delta / hour\n",
    "    if 0 < delta and delta <= _hours:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def generate(classifier_generator, config, saveas):\n",
    "    classifier, free_model = classifier_generator(config)\n",
    "    with open(\"BTC_USDT-4h_interval.json\") as f: \n",
    "        d = json.load(f) \n",
    "\n",
    "    chart_df = pd.DataFrame(d) \n",
    "    chart_df = chart_df.rename(columns={0:\"timestamp\",\n",
    "                                        1:\"open\",\n",
    "                                        2:\"high\",\n",
    "                                        3:\"low\", \n",
    "                                        4:\"close\",\n",
    "                                        5:\"volume\"}) \n",
    "\n",
    "    chart_df = process(chart_df) \n",
    "\n",
    "    hours, days, months, years = [],[],[],[] \n",
    "\n",
    "    for dt in tqdm(chart_df['datetime']):\n",
    "        dtobj = pd.to_datetime(dt) \n",
    "        hour = dtobj.hour \n",
    "        day = dtobj.day \n",
    "        month = dtobj.month \n",
    "        year = dtobj.year \n",
    "        hours.append(hour)\n",
    "        days.append(day) \n",
    "        months.append(month)\n",
    "        years.append(year) \n",
    "    \n",
    "    chart_df['years'], chart_df['months'], chart_df['days'], chart_df['hours'] = years, months, days, hours\n",
    "    \n",
    "    high_change, low_change = [], [] \n",
    "    close = chart_df['close'].values \n",
    "    high = chart_df['high'].values \n",
    "    low = chart_df['low'].values \n",
    "    for i in range(close.shape[0]-1):\n",
    "        high_delta = (high[i+1] - close[i]) / close[i] \n",
    "        low_delta = (low[i+1] - close[i]) / close[i]\n",
    "        high_change.append(high_delta) \n",
    "        low_change.append(low_delta)\n",
    "    high_change.append(None) \n",
    "    low_change.append(None)\n",
    "\n",
    "    chart_df['high_delta'] = high_change\n",
    "    chart_df['low_delta'] = low_change \n",
    "\n",
    "    chart_df['sent_0'] = 0.0\n",
    "    chart_df['sent_1'] = 0.0\n",
    "    chart_df['sent_2'] = 0.0\n",
    "\n",
    "    chart_df.dropna(inplace=True) \n",
    "    \n",
    "    news = {}\n",
    "    news_file=\"full_news_labeled.csv\"\n",
    "    with open(news_file) as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        for i, row in tqdm(enumerate(reader)):\n",
    "            if i == 0:\n",
    "                columns = row\n",
    "                print(columns)\n",
    "                continue\n",
    "            date = f\"{row[3]}-{row[4]}-{row[5]}-{row[6]}\"\n",
    "            text = f\"{row[0]}. {row[1]}\"\n",
    "\n",
    "            res_vec = classifier(row[0], row[1])\n",
    "            #res_vec = res2vec(res)\n",
    "\n",
    "            mask = [0, 0, 0]\n",
    "            mask[int(row[2])] = 1\n",
    "\n",
    "            if date in news:\n",
    "                news[date][0] += 1\n",
    "                news[date][1] = vec_sum(news[date][1], res_vec)\n",
    "                news[date][2] = vec_sum(news[date][2], mask)\n",
    "            else:\n",
    "                news[date] = [1, res_vec, mask]\n",
    "\n",
    "            #print(date, news[date])\n",
    "            #break\n",
    "    \n",
    "    for index, row in tqdm(chart_df.iterrows()):\n",
    "        query = datetime.datetime(row['years'], row['months'], row['days'], row['hours'])\n",
    "        lk = list(news.keys())\n",
    "        idx = closest(query, lk)\n",
    "        ks = []\n",
    "        n = 0\n",
    "        \n",
    "        for ik in range(idx-1, -1, -1):\n",
    "            k = lk[ik]\n",
    "            if isin(query, k):\n",
    "                ks.append(k)\n",
    "                n += news[k][0]\n",
    "            else:\n",
    "                break\n",
    "        for k in ks:\n",
    "            try:\n",
    "                v = news[k][1].cpu().detach().numpy()\n",
    "            except:\n",
    "                v = news[k][1]\n",
    "                for i in range(3):\n",
    "                    if not isinstance(v[i], numpy.ndarray):\n",
    "                        v[i] = v[i].cpu().detach().numpy()\n",
    "            chart_df.at[index, 'sent_0'] += v[0] / n\n",
    "            chart_df.at[index, 'sent_1'] += v[1] / n\n",
    "            chart_df.at[index, 'sent_2'] += v[2] / n\n",
    "            \n",
    "    chart_df.set_index(pd.DatetimeIndex(chart_df['datetime']), inplace=True) \n",
    "\n",
    "    chart_df['bop'] = chart_df.ta.bop(lookahead=False)\n",
    "    chart_df['ebsw'] = chart_df.ta.ebsw(lookahead=False) \n",
    "    chart_df['cmf'] = chart_df.ta.cmf(lookahead=False) \n",
    "    chart_df['rsi/100'] = chart_df.ta.rsi(lookahead=False) / 100 \n",
    "    chart_df['vwap'] = chart_df.ta.vwap(lookahead=False) \n",
    "    chart_df['high/low'] = chart_df['high'] / chart_df['low'] \n",
    "    chart_df['close/open'] = chart_df['close'] / chart_df['open'] \n",
    "    chart_df['high/open'] = chart_df['high'] / chart_df['open'] \n",
    "    chart_df['low/open'] = chart_df['low'] / chart_df['open'] \n",
    "\n",
    "    chart_df['hwma'] = chart_df.ta.hwma(lookahead=False)\n",
    "    chart_df['linreg'] = chart_df.ta.linreg(lookahead=False)\n",
    "    chart_df['hwma/close'] = chart_df['hwma'] / chart_df['close'] \n",
    "    chart_df['linreg/close'] = chart_df['linreg'] / chart_df['close']\n",
    "\n",
    "    for i in tqdm(range(1, 4)): \n",
    "        for col in ['open', 'high', 'low', 'close', 'volume', 'vwap']:\n",
    "            val = chart_df[col].values \n",
    "            val_ret = [None for _ in range(i)] \n",
    "            for j in range(i, len(val)): \n",
    "                if val[j-i] == 0:\n",
    "                    ret = 1 \n",
    "                else:\n",
    "                    ret = val[j] / val[j-i] \n",
    "                val_ret.append(ret) \n",
    "            chart_df['{}_change_{}'.format(col, i)] = val_ret \n",
    "\n",
    "\n",
    "\n",
    "    chart_df.dropna(inplace=True) \n",
    "    chart_df.drop(columns={'datetime', 'open', 'high', 'low', 'close', 'volume', 'vwap', 'hwma', 'linreg', 'years'}, inplace=True) \n",
    "    \n",
    "    chart_df.to_csv(saveas)\n",
    "    free_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c358d24",
   "metadata": {},
   "source": [
    "## Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60944dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "100%|█████████████████████████████████████████████████████████████| 10560/10560 [00:01<00:00, 6479.67it/s]\n",
      "4it [00:00, 30.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['title', 'content', 'labels', 'year', 'month', 'day', 'hour']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10550it [06:54, 24.61it/s]"
     ]
    }
   ],
   "source": [
    "for name, config in configs.items():\n",
    "    savedir = opj(\"data\", f\"{sentiment_name}\")\n",
    "    if not os.path.exists(savedir):\n",
    "        os.makedirs(savedir, exist_ok=True)\n",
    "    saveas = opj(\"data\", f\"{sentiment_name}\", f\"{name}.csv\")\n",
    "    generate(classifier_generator, config, saveas)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
