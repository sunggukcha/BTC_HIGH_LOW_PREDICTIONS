{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a30df4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "import ccxt\n",
    "import pandas as pd\n",
    "import pandas_ta as ta # needed for processing chart data \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "from tokenization_roberta_spm import FairSeqRobertaSentencePieceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60944dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RobertaTokenizer'. \n",
      "The class this function is called from is 'FairSeqRobertaSentencePieceTokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PASS] spm_id: madeupword0000 | fairseq_id: 51998\n",
      "[PASS] spm_id: madeupword0001 | fairseq_id: 51999\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XLMRobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(52001, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"totoro4007/cryptoroberta-base-all-finetuned\"\n",
    "\n",
    "tokenizer = FairSeqRobertaSentencePieceTokenizer.from_pretrained(\"fairseq-roberta-all-model\")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "device = torch.device('cuda')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4044b29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier(title, content):\n",
    "    encoded_inputs = tokenizer(title, content, return_tensors=\"pt\", max_length=512, truncation=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model(**encoded_inputs)\n",
    "        logits = output['logits']\n",
    "    res = nn.Softmax(dim=1)(logits)[0]\n",
    "    return res\n",
    "\n",
    "def res2vec(res):\n",
    "    vec = [0, 0, 0]\n",
    "    for r in res:\n",
    "        label = int(r['label'].split('_')[-1])\n",
    "        score = round(r['score'], 4)\n",
    "        vec[label] = score\n",
    "    return vec\n",
    "\n",
    "def vec_sum(vec1, vec2):\n",
    "    return [vec1[0]+vec2[0], vec1[1]+vec2[1], vec1[2]+vec2[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f991d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "6it [00:00, 57.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['title', 'content', 'labels', 'year', 'month', 'day', 'hour']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [00:00, 56.66it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "90it [00:01, 55.27it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "114it [00:02, 54.04it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "452it [00:07, 64.01it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "859it [00:13, 65.27it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "1263it [00:20, 55.28it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "1729it [00:27, 55.04it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "1741it [00:28, 52.77it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "1765it [00:28, 52.51it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "1777it [00:28, 50.68it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "1789it [00:29, 51.20it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "1825it [00:29, 54.18it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "1849it [00:30, 54.11it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "1874it [00:30, 54.74it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "1880it [00:30, 54.05it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "2019it [00:32, 65.41it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "2054it [00:33, 66.13it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "2068it [00:33, 65.81it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "2222it [00:36, 64.19it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "2306it [00:37, 60.70it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "2432it [00:39, 65.09it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "2726it [00:43, 61.49it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "2747it [00:44, 63.16it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "2995it [00:48, 66.08it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "3058it [00:49, 65.64it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "3371it [00:54, 61.37it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "3527it [00:57, 54.01it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "3881it [01:03, 53.88it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "4048it [01:06, 67.88it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "4118it [01:07, 66.36it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "4294it [01:10, 66.76it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "4371it [01:11, 65.21it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4476it [01:13, 66.78it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "4575it [01:14, 67.70it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "4596it [01:15, 65.15it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "4639it [01:15, 64.85it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "4710it [01:16, 66.03it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "4724it [01:17, 64.01it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "5092it [01:22, 54.49it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "5450it [01:28, 64.63it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "5977it [01:36, 67.27it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "6166it [01:39, 67.24it/s]"
     ]
    }
   ],
   "source": [
    "news = {}\n",
    "news_file=\"full_news_labeled.csv\"\n",
    "with open(news_file) as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    for i, row in tqdm(enumerate(reader)):\n",
    "        if i == 0:\n",
    "            columns = row\n",
    "            print(columns)\n",
    "            continue\n",
    "        date = f\"{row[3]}-{row[4]}-{row[5]}-{row[6]}\"\n",
    "        text = f\"{row[0]}. {row[1]}\"\n",
    "        \n",
    "        res_vec = classifier(row[0], row[1])\n",
    "        #res_vec = res2vec(res)\n",
    "        \n",
    "        mask = [0, 0, 0]\n",
    "        mask[int(row[2])] = 1\n",
    "        \n",
    "        if date in news:\n",
    "            news[date][0] += 1\n",
    "            news[date][1] = vec_sum(news[date][1], res_vec)\n",
    "            news[date][2] = vec_sum(news[date][2], mask)\n",
    "        else:\n",
    "            news[date] = [1, res_vec, mask]\n",
    "        \n",
    "        #print(date, news[date])\n",
    "        #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7173f08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dates2datetimes(dates):\n",
    "    res = []\n",
    "    for date in dates:\n",
    "        date = list(map(int, date.split('-')))\n",
    "        neo = datetime.datetime(date[0], date[1], date[2], date[3])\n",
    "        res.append(neo)\n",
    "    return res\n",
    "\n",
    "def closest(query, dates):\n",
    "    dates = dates2datetimes(dates)\n",
    "    st = 0\n",
    "    en = len(dates)-1\n",
    "    while True:\n",
    "        mid = (st + en) // 2\n",
    "        if dates[mid] < query:\n",
    "            st = mid\n",
    "        else:\n",
    "            en = mid\n",
    "        if mid == (st + en) // 2:\n",
    "            break            \n",
    "    return mid\n",
    "\n",
    "def isin(query, date):\n",
    "    date = list(map(int, date.split('-')))\n",
    "    date = datetime.datetime(date[0], date[1], date[2], date[3])\n",
    "    hour = datetime.timedelta(hours=1)\n",
    "    delta = query - date\n",
    "    delta = delta / hour\n",
    "    if 0 < delta and delta <= 4:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe93407",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"BTC_USDT-4h_interval.json\") as f: \n",
    "    d = json.load(f) \n",
    "    \n",
    "chart_df = pd.DataFrame(d) \n",
    "chart_df = chart_df.rename(columns={0:\"timestamp\",\n",
    "                                    1:\"open\",\n",
    "                                    2:\"high\",\n",
    "                                    3:\"low\", \n",
    "                                    4:\"close\",\n",
    "                                    5:\"volume\"}) \n",
    "\n",
    "def process(df):\n",
    "    binance = ccxt.binance() \n",
    "    dates = df['timestamp'].values \n",
    "    timestamp = [] \n",
    "    for i in range(len(dates)):\n",
    "        date_string = binance.iso8601(int(dates[i])) \n",
    "        date_string = date_string[:10] + \" \" + date_string[11:-5] \n",
    "        timestamp.append(date_string) \n",
    "    df['datetime'] = timestamp \n",
    "    df = df.drop(columns={'timestamp'}) \n",
    "    return df\n",
    "\n",
    "chart_df = process(chart_df) \n",
    "\n",
    "hours, days, months, years = [],[],[],[] \n",
    "\n",
    "for dt in tqdm(chart_df['datetime']):\n",
    "    dtobj = pd.to_datetime(dt) \n",
    "    hour = dtobj.hour \n",
    "    day = dtobj.day \n",
    "    month = dtobj.month \n",
    "    year = dtobj.year \n",
    "    hours.append(hour)\n",
    "    days.append(day) \n",
    "    months.append(month)\n",
    "    years.append(year) \n",
    "    \n",
    "chart_df['years'], chart_df['months'], chart_df['days'], chart_df['hours'] = years, months, days, hours\n",
    "    \n",
    "high_change, low_change = [], [] \n",
    "close = chart_df['close'].values \n",
    "high = chart_df['high'].values \n",
    "low = chart_df['low'].values \n",
    "for i in range(close.shape[0]-1):\n",
    "    high_delta = (high[i+1] - close[i]) / close[i] \n",
    "    low_delta = (low[i+1] - close[i]) / close[i]\n",
    "    high_change.append(high_delta) \n",
    "    low_change.append(low_delta)\n",
    "high_change.append(None) \n",
    "low_change.append(None)\n",
    "\n",
    "chart_df['high_delta'] = high_change\n",
    "chart_df['low_delta'] = low_change \n",
    "\n",
    "chart_df['sent_0'] = 0.0\n",
    "chart_df['sent_1'] = 0.0\n",
    "chart_df['sent_2'] = 0.0\n",
    "\n",
    "chart_df.dropna(inplace=True) \n",
    "\n",
    "chart_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1dd111",
   "metadata": {},
   "outputs": [],
   "source": [
    "chart_df.iloc[10199]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84096068",
   "metadata": {},
   "outputs": [],
   "source": [
    "news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd7e3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in tqdm(chart_df.iterrows()):\n",
    "    query = datetime.datetime(row['years'], row['months'], row['days'], row['hours'])\n",
    "    lk = list(news.keys())\n",
    "    idx = closest(query, lk)\n",
    "    ks = []\n",
    "    n = 0\n",
    "    for k in lk[max(0, idx-5):idx]:\n",
    "        if isin(query, k):\n",
    "            ks.append(k)\n",
    "            n += news[k][0]\n",
    "    for k in ks:\n",
    "        try:\n",
    "            v = news[k][1].cpu().detach().numpy()\n",
    "        except:\n",
    "            v = news[k][1]\n",
    "            for i in range(3):\n",
    "                v[i] = v[i].cpu().detach().numpy()\n",
    "        chart_df.at[index, 'sent_0'] += v[0] / n\n",
    "        chart_df.at[index, 'sent_1'] += v[1] / n\n",
    "        chart_df.at[index, 'sent_2'] += v[2] / n\n",
    "        \n",
    "chart_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089d7a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(chart_df.iloc[10199])\n",
    "for index, row in tqdm(chart_df.iterrows()):\n",
    "    if row.sent_0 > 0:\n",
    "        print(row)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2d6214",
   "metadata": {},
   "outputs": [],
   "source": [
    "chart_df.set_index(pd.DatetimeIndex(chart_df['datetime']), inplace=True) \n",
    "\n",
    "chart_df['bop'] = chart_df.ta.bop(lookahead=False)\n",
    "chart_df['ebsw'] = chart_df.ta.ebsw(lookahead=False) \n",
    "chart_df['cmf'] = chart_df.ta.cmf(lookahead=False) \n",
    "chart_df['rsi/100'] = chart_df.ta.rsi(lookahead=False) / 100 \n",
    "chart_df['vwap'] = chart_df.ta.vwap(lookahead=False) \n",
    "chart_df['high/low'] = chart_df['high'] / chart_df['low'] \n",
    "chart_df['close/open'] = chart_df['close'] / chart_df['open'] \n",
    "chart_df['high/open'] = chart_df['high'] / chart_df['open'] \n",
    "chart_df['low/open'] = chart_df['low'] / chart_df['open'] \n",
    "\n",
    "chart_df['hwma'] = chart_df.ta.hwma(lookahead=False)\n",
    "chart_df['linreg'] = chart_df.ta.linreg(lookahead=False)\n",
    "chart_df['hwma/close'] = chart_df['hwma'] / chart_df['close'] \n",
    "chart_df['linreg/close'] = chart_df['linreg'] / chart_df['close']\n",
    "\n",
    "for i in tqdm(range(1, 4)): \n",
    "    for col in ['open', 'high', 'low', 'close', 'volume', 'vwap']:\n",
    "        val = chart_df[col].values \n",
    "        val_ret = [None for _ in range(i)] \n",
    "        for j in range(i, len(val)): \n",
    "            if val[j-i] == 0:\n",
    "                ret = 1 \n",
    "            else:\n",
    "                ret = val[j] / val[j-i] \n",
    "            val_ret.append(ret) \n",
    "        chart_df['{}_change_{}'.format(col, i)] = val_ret \n",
    "            \n",
    "    \n",
    "\n",
    "chart_df.dropna(inplace=True) \n",
    "chart_df.drop(columns={'datetime', 'open', 'high', 'low', 'close', 'volume', 'vwap', 'hwma', 'linreg', 'years'}, inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c076b566",
   "metadata": {},
   "outputs": [],
   "source": [
    "chart_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b61656",
   "metadata": {},
   "outputs": [],
   "source": [
    "chart_df.to_csv('roberta.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
