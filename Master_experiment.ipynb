{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "opj = os.path.join\n",
    "\n",
    "import ccxt \n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd \n",
    "from pytorch_tabnet.tab_model import TabNetClassifier, TabNetRegressor \n",
    "from sklearn.metrics import f1_score, mean_squared_error, mean_absolute_error\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_files = [\n",
    "    \"crypto-gt.csv\",\n",
    "    \"deberta.csv\",\n",
    "    \"default.csv\",\n",
    "    \"roberta.csv\",\n",
    "    \"bert.csv\"\n",
    "]\n",
    "\n",
    "results = {}\n",
    "saveas = \"experiments.csv\"\n",
    "checkpoint_save_dir = \"ckpts\"\n",
    "exp_times = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(df_file, N):\n",
    "    exp_name = df_file.split(\".\")[0]\n",
    "    # load data\n",
    "    chart_df = pd.read_csv(df_file)\n",
    "    i1 = 0\n",
    "    i2 = len(chart_df)-1\n",
    "    st = \"2018-02-14 14:00:00\"\n",
    "    en = \"2022-04-15 23:00:00\"\n",
    "    filt = []\n",
    "    for index, row in chart_df.iterrows():\n",
    "        if chart_df.iloc[i1]['datetime'] < row.datetime and row.datetime < st:\n",
    "            i1 = index\n",
    "        if en < row.datetime and row.datetime < chart_df.iloc[i2]['datetime']:\n",
    "            i2 = index\n",
    "    chart_df = chart_df[i1+1:i2]\n",
    "    chart_df.index = range(len(chart_df))\n",
    "    \n",
    "    # Train High Model\n",
    "    ## train/validation/test split \n",
    "    train_size = int(chart_df.shape[0] * 0.8) \n",
    "    train_df = chart_df.iloc[:train_size,:] \n",
    "\n",
    "    val_size = int(chart_df.shape[0] * 0.1) \n",
    "    val_df = chart_df.iloc[train_size:train_size+val_size,:]  \n",
    "\n",
    "    test_df = chart_df.iloc[train_size+val_size:, :] \n",
    "    \n",
    "    categorical_columns = [\"months\", \"days\", \"hours\"]\n",
    "    features = train_df.columns\n",
    "\n",
    "    cat_idxs = [0, 1, 2] \n",
    "    cat_dims = [13, 32, 25] \n",
    "\n",
    "    tabnet_params = {\"cat_idxs\":cat_idxs, \n",
    "                     \"cat_dims\":cat_dims, \n",
    "                     \"cat_emb_dim\":1, \n",
    "                     \"optimizer_fn\":torch.optim.Adam} \n",
    "    \n",
    "    input_columns = [] \n",
    "    for col in train_df.columns:\n",
    "        if col != 'low_delta' and col != 'years' and col != 'datetime': \n",
    "            input_columns.append(col) \n",
    "\n",
    "    X_train = train_df[input_columns].values \n",
    "    Y_train = train_df['high_delta'].values \n",
    "    Y_train = Y_train.reshape((-1,1))\n",
    "\n",
    "    X_val = val_df[input_columns].values\n",
    "    Y_val = val_df['high_delta'].values \n",
    "    Y_val = Y_val.reshape((-1,1))\n",
    "\n",
    "    X_test = test_df[input_columns].values \n",
    "    Y_test = test_df['high_delta'].values  \n",
    "    Y_test = Y_test.reshape((-1,1))\n",
    "    \n",
    "    reg_high = TabNetRegressor(**tabnet_params) \n",
    "\n",
    "    reg_high.fit(X_train, Y_train, \n",
    "                 eval_set=[(X_val, Y_val)], \n",
    "                 max_epochs=100, \n",
    "                 patience=100)  \n",
    "    \n",
    "    Y_pred = reg_high.predict(X_test).flatten() \n",
    "    \n",
    "    exp = f\"{exp_name}-high\"\n",
    "    if exp not in results:\n",
    "        results[exp] = []\n",
    "        \n",
    "    results[exp].append(\n",
    "        {\n",
    "            \"MSE\": mean_squared_error(Y_test, Y_pred),\n",
    "            \"MAE\": mean_absolute_error(Y_test, Y_pred)\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    saveas = opj(checkpoint_save_dir, f\"{exp}-N\")\n",
    "    reg_high.save_model(saveas)\n",
    "    \n",
    "    # Train Low Model\n",
    "    input_columns = [] \n",
    "    for col in train_df.columns:\n",
    "        if col != 'high_delta' and col != 'years' and col != 'datetime': \n",
    "            input_columns.append(col) \n",
    "\n",
    "    X_train = train_df[input_columns].values \n",
    "    Y_train = train_df['low_delta'].values \n",
    "    Y_train = Y_train.reshape((-1,1))\n",
    "\n",
    "    X_val = val_df[input_columns].values\n",
    "    Y_val = val_df['low_delta'].values \n",
    "    Y_val = Y_val.reshape((-1,1))\n",
    "\n",
    "    X_test = test_df[input_columns].values \n",
    "    Y_test = test_df['low_delta'].values  \n",
    "    Y_test = Y_test.reshape((-1,1))\n",
    "    \n",
    "    reg_low = TabNetRegressor(**tabnet_params) \n",
    "\n",
    "    reg_low.fit(X_train, Y_train, \n",
    "                eval_set=[(X_val, Y_val)], \n",
    "                max_epochs=200, \n",
    "                patience=200)  \n",
    "    \n",
    "    Y_pred = reg_low.predict(X_test).flatten() \n",
    "    exp = f\"{exp_name}-low\"\n",
    "    if exp not in results:\n",
    "        results[exp] = []\n",
    "        \n",
    "    results[exp].append(\n",
    "        {\n",
    "            \"MSE\": mean_squared_error(Y_test, Y_pred),\n",
    "            \"MAE\": mean_absolute_error(Y_test, Y_pred)\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    saveas = opj(checkpoint_save_dir, f\"{exp}-{N}\")\n",
    "    reg_high.save_model(saveas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device used : cuda\n",
      "epoch 0  | loss: 0.30603 | val_0_mse: 0.00768 |  0:00:00s\n",
      "epoch 1  | loss: 0.05294 | val_0_mse: 0.00075 |  0:00:01s\n",
      "epoch 2  | loss: 0.013   | val_0_mse: 0.00038 |  0:00:02s\n",
      "epoch 3  | loss: 0.00455 | val_0_mse: 0.00042 |  0:00:02s\n",
      "epoch 4  | loss: 0.00227 | val_0_mse: 0.00023 |  0:00:03s\n",
      "epoch 5  | loss: 0.00152 | val_0_mse: 0.00018 |  0:00:03s\n",
      "epoch 6  | loss: 0.00132 | val_0_mse: 0.00016 |  0:00:04s\n",
      "epoch 7  | loss: 0.00129 | val_0_mse: 0.00017 |  0:00:05s\n",
      "epoch 8  | loss: 0.00121 | val_0_mse: 0.00016 |  0:00:05s\n",
      "epoch 9  | loss: 0.00119 | val_0_mse: 0.00017 |  0:00:06s\n",
      "epoch 10 | loss: 0.00116 | val_0_mse: 0.00017 |  0:00:07s\n",
      "epoch 11 | loss: 0.00114 | val_0_mse: 0.00015 |  0:00:08s\n",
      "epoch 12 | loss: 0.00113 | val_0_mse: 0.00016 |  0:00:09s\n",
      "epoch 13 | loss: 0.00114 | val_0_mse: 0.00015 |  0:00:09s\n",
      "epoch 14 | loss: 0.0011  | val_0_mse: 0.00017 |  0:00:10s\n",
      "epoch 15 | loss: 0.00109 | val_0_mse: 0.00015 |  0:00:11s\n",
      "epoch 16 | loss: 0.00111 | val_0_mse: 0.00015 |  0:00:11s\n",
      "epoch 17 | loss: 0.00109 | val_0_mse: 0.00016 |  0:00:12s\n",
      "epoch 18 | loss: 0.00106 | val_0_mse: 0.00015 |  0:00:13s\n",
      "epoch 19 | loss: 0.00104 | val_0_mse: 0.00015 |  0:00:13s\n",
      "epoch 20 | loss: 0.00105 | val_0_mse: 0.00016 |  0:00:14s\n",
      "epoch 21 | loss: 0.00105 | val_0_mse: 0.00016 |  0:00:14s\n",
      "epoch 22 | loss: 0.00103 | val_0_mse: 0.00016 |  0:00:15s\n",
      "epoch 23 | loss: 0.00102 | val_0_mse: 0.00015 |  0:00:16s\n",
      "epoch 24 | loss: 0.00102 | val_0_mse: 0.00015 |  0:00:17s\n",
      "epoch 25 | loss: 0.00101 | val_0_mse: 0.00014 |  0:00:18s\n",
      "epoch 26 | loss: 0.00098 | val_0_mse: 0.00014 |  0:00:18s\n",
      "epoch 27 | loss: 0.00099 | val_0_mse: 0.00014 |  0:00:19s\n",
      "epoch 28 | loss: 0.00097 | val_0_mse: 0.00014 |  0:00:20s\n",
      "epoch 29 | loss: 0.00096 | val_0_mse: 0.00014 |  0:00:20s\n",
      "epoch 30 | loss: 0.00095 | val_0_mse: 0.00014 |  0:00:21s\n",
      "epoch 31 | loss: 0.00095 | val_0_mse: 0.00014 |  0:00:22s\n",
      "epoch 32 | loss: 0.00095 | val_0_mse: 0.00014 |  0:00:22s\n",
      "epoch 33 | loss: 0.00093 | val_0_mse: 0.00014 |  0:00:23s\n",
      "epoch 34 | loss: 0.00091 | val_0_mse: 0.00014 |  0:00:24s\n",
      "epoch 35 | loss: 0.0009  | val_0_mse: 0.00014 |  0:00:24s\n",
      "epoch 36 | loss: 0.0009  | val_0_mse: 0.00014 |  0:00:25s\n",
      "epoch 37 | loss: 0.00089 | val_0_mse: 0.00014 |  0:00:26s\n",
      "epoch 38 | loss: 0.00088 | val_0_mse: 0.00015 |  0:00:27s\n",
      "epoch 39 | loss: 0.00087 | val_0_mse: 0.00016 |  0:00:27s\n",
      "epoch 40 | loss: 0.00086 | val_0_mse: 0.00014 |  0:00:28s\n",
      "epoch 41 | loss: 0.00086 | val_0_mse: 0.00015 |  0:00:28s\n",
      "epoch 42 | loss: 0.00085 | val_0_mse: 0.00014 |  0:00:29s\n",
      "epoch 43 | loss: 0.00084 | val_0_mse: 0.00014 |  0:00:30s\n",
      "epoch 44 | loss: 0.00082 | val_0_mse: 0.00014 |  0:00:30s\n",
      "epoch 45 | loss: 0.00082 | val_0_mse: 0.00014 |  0:00:31s\n",
      "epoch 46 | loss: 0.00081 | val_0_mse: 0.00015 |  0:00:32s\n",
      "epoch 47 | loss: 0.0008  | val_0_mse: 0.00013 |  0:00:33s\n",
      "epoch 48 | loss: 0.00079 | val_0_mse: 0.00014 |  0:00:33s\n",
      "epoch 49 | loss: 0.0008  | val_0_mse: 0.00013 |  0:00:34s\n",
      "epoch 50 | loss: 0.00079 | val_0_mse: 0.00013 |  0:00:35s\n",
      "epoch 51 | loss: 0.00079 | val_0_mse: 0.00014 |  0:00:36s\n",
      "epoch 52 | loss: 0.00078 | val_0_mse: 0.00013 |  0:00:36s\n",
      "epoch 53 | loss: 0.00077 | val_0_mse: 0.00015 |  0:00:37s\n",
      "epoch 54 | loss: 0.00077 | val_0_mse: 0.00016 |  0:00:38s\n",
      "epoch 55 | loss: 0.00077 | val_0_mse: 0.00014 |  0:00:38s\n",
      "epoch 56 | loss: 0.00075 | val_0_mse: 0.00013 |  0:00:39s\n",
      "epoch 57 | loss: 0.00074 | val_0_mse: 0.00015 |  0:00:40s\n",
      "epoch 58 | loss: 0.00074 | val_0_mse: 0.00013 |  0:00:40s\n",
      "epoch 59 | loss: 0.00074 | val_0_mse: 0.00013 |  0:00:41s\n",
      "epoch 60 | loss: 0.00073 | val_0_mse: 0.00016 |  0:00:42s\n",
      "epoch 61 | loss: 0.00072 | val_0_mse: 0.00013 |  0:00:42s\n",
      "epoch 62 | loss: 0.00073 | val_0_mse: 0.00013 |  0:00:43s\n",
      "epoch 63 | loss: 0.00072 | val_0_mse: 0.00015 |  0:00:44s\n",
      "epoch 64 | loss: 0.0007  | val_0_mse: 0.00014 |  0:00:44s\n",
      "epoch 65 | loss: 0.00069 | val_0_mse: 0.00014 |  0:00:45s\n",
      "epoch 66 | loss: 0.00069 | val_0_mse: 0.00013 |  0:00:46s\n",
      "epoch 67 | loss: 0.00068 | val_0_mse: 0.00013 |  0:00:46s\n",
      "epoch 68 | loss: 0.00067 | val_0_mse: 0.00014 |  0:00:47s\n",
      "epoch 69 | loss: 0.00067 | val_0_mse: 0.00013 |  0:00:48s\n",
      "epoch 70 | loss: 0.00066 | val_0_mse: 0.00013 |  0:00:48s\n",
      "epoch 71 | loss: 0.00065 | val_0_mse: 0.00013 |  0:00:49s\n",
      "epoch 72 | loss: 0.00064 | val_0_mse: 0.00014 |  0:00:50s\n",
      "epoch 73 | loss: 0.00064 | val_0_mse: 0.00013 |  0:00:50s\n",
      "epoch 74 | loss: 0.00064 | val_0_mse: 0.00014 |  0:00:51s\n",
      "epoch 75 | loss: 0.00063 | val_0_mse: 0.00013 |  0:00:52s\n",
      "epoch 76 | loss: 0.00062 | val_0_mse: 0.00013 |  0:00:52s\n",
      "epoch 77 | loss: 0.00062 | val_0_mse: 0.00015 |  0:00:53s\n",
      "epoch 78 | loss: 0.00062 | val_0_mse: 0.00014 |  0:00:53s\n",
      "epoch 79 | loss: 0.00063 | val_0_mse: 0.00014 |  0:00:54s\n",
      "epoch 80 | loss: 0.0006  | val_0_mse: 0.00014 |  0:00:55s\n",
      "epoch 81 | loss: 0.0006  | val_0_mse: 0.00013 |  0:00:56s\n",
      "epoch 82 | loss: 0.00058 | val_0_mse: 0.00013 |  0:00:56s\n",
      "epoch 83 | loss: 0.00059 | val_0_mse: 0.00014 |  0:00:57s\n",
      "epoch 84 | loss: 0.00058 | val_0_mse: 0.00013 |  0:00:58s\n",
      "epoch 85 | loss: 0.00057 | val_0_mse: 0.00013 |  0:00:58s\n",
      "epoch 86 | loss: 0.00057 | val_0_mse: 0.00013 |  0:00:59s\n",
      "epoch 87 | loss: 0.00056 | val_0_mse: 0.00013 |  0:01:00s\n",
      "epoch 88 | loss: 0.00056 | val_0_mse: 0.00013 |  0:01:01s\n",
      "epoch 89 | loss: 0.00056 | val_0_mse: 0.00013 |  0:01:01s\n",
      "epoch 90 | loss: 0.00055 | val_0_mse: 0.00013 |  0:01:02s\n",
      "epoch 91 | loss: 0.00055 | val_0_mse: 0.00013 |  0:01:03s\n",
      "epoch 92 | loss: 0.00055 | val_0_mse: 0.00013 |  0:01:03s\n",
      "epoch 93 | loss: 0.00054 | val_0_mse: 0.00013 |  0:01:04s\n",
      "epoch 94 | loss: 0.00054 | val_0_mse: 0.00013 |  0:01:04s\n",
      "epoch 95 | loss: 0.00054 | val_0_mse: 0.00013 |  0:01:05s\n",
      "epoch 96 | loss: 0.00053 | val_0_mse: 0.00013 |  0:01:06s\n",
      "epoch 97 | loss: 0.00053 | val_0_mse: 0.00013 |  0:01:06s\n",
      "epoch 98 | loss: 0.00052 | val_0_mse: 0.00013 |  0:01:07s\n",
      "epoch 99 | loss: 0.00052 | val_0_mse: 0.00015 |  0:01:08s\n",
      "Stop training because you reached max_epochs = 100 with best_epoch = 92 and best_val_0_mse = 0.00013\n",
      "Best weights from best epoch are automatically used!\n",
      "Successfully saved model at ckpts/crypto-gt-high-N.zip\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.35515 | val_0_mse: 0.05439 |  0:00:00s\n",
      "epoch 1  | loss: 0.05881 | val_0_mse: 0.00308 |  0:00:01s\n",
      "epoch 2  | loss: 0.01764 | val_0_mse: 0.00073 |  0:00:01s\n",
      "epoch 3  | loss: 0.00615 | val_0_mse: 0.00052 |  0:00:02s\n",
      "epoch 4  | loss: 0.00339 | val_0_mse: 0.00024 |  0:00:03s\n",
      "epoch 5  | loss: 0.00221 | val_0_mse: 0.00017 |  0:00:04s\n",
      "epoch 6  | loss: 0.00164 | val_0_mse: 0.00019 |  0:00:04s\n",
      "epoch 7  | loss: 0.0015  | val_0_mse: 0.00017 |  0:00:05s\n",
      "epoch 8  | loss: 0.00153 | val_0_mse: 0.00017 |  0:00:06s\n",
      "epoch 9  | loss: 0.00132 | val_0_mse: 0.00016 |  0:00:06s\n",
      "epoch 10 | loss: 0.00128 | val_0_mse: 0.00017 |  0:00:07s\n",
      "epoch 11 | loss: 0.00124 | val_0_mse: 0.00016 |  0:00:08s\n",
      "epoch 12 | loss: 0.00123 | val_0_mse: 0.00016 |  0:00:08s\n",
      "epoch 13 | loss: 0.00122 | val_0_mse: 0.00015 |  0:00:09s\n",
      "epoch 14 | loss: 0.00117 | val_0_mse: 0.00015 |  0:00:10s\n",
      "epoch 15 | loss: 0.00117 | val_0_mse: 0.00014 |  0:00:11s\n",
      "epoch 16 | loss: 0.00113 | val_0_mse: 0.00015 |  0:00:12s\n",
      "epoch 17 | loss: 0.00111 | val_0_mse: 0.00014 |  0:00:12s\n",
      "epoch 18 | loss: 0.0011  | val_0_mse: 0.00015 |  0:00:13s\n",
      "epoch 19 | loss: 0.00109 | val_0_mse: 0.00014 |  0:00:14s\n",
      "epoch 20 | loss: 0.00106 | val_0_mse: 0.00013 |  0:00:15s\n",
      "epoch 21 | loss: 0.00105 | val_0_mse: 0.00013 |  0:00:16s\n",
      "epoch 22 | loss: 0.00104 | val_0_mse: 0.00014 |  0:00:16s\n",
      "epoch 23 | loss: 0.00104 | val_0_mse: 0.00014 |  0:00:17s\n",
      "epoch 24 | loss: 0.00099 | val_0_mse: 0.00014 |  0:00:18s\n",
      "epoch 25 | loss: 0.00099 | val_0_mse: 0.00013 |  0:00:19s\n",
      "epoch 26 | loss: 0.00096 | val_0_mse: 0.00013 |  0:00:20s\n",
      "epoch 27 | loss: 0.00094 | val_0_mse: 0.00014 |  0:00:21s\n",
      "epoch 28 | loss: 0.00094 | val_0_mse: 0.00013 |  0:00:21s\n",
      "epoch 29 | loss: 0.00095 | val_0_mse: 0.00013 |  0:00:22s\n",
      "epoch 30 | loss: 0.0009  | val_0_mse: 0.00013 |  0:00:23s\n",
      "epoch 31 | loss: 0.00089 | val_0_mse: 0.00013 |  0:00:24s\n",
      "epoch 32 | loss: 0.0009  | val_0_mse: 0.00013 |  0:00:25s\n",
      "epoch 33 | loss: 0.00084 | val_0_mse: 0.00013 |  0:00:26s\n",
      "epoch 34 | loss: 0.00083 | val_0_mse: 0.00013 |  0:00:27s\n",
      "epoch 35 | loss: 0.00081 | val_0_mse: 0.00013 |  0:00:28s\n",
      "epoch 36 | loss: 0.0008  | val_0_mse: 0.00013 |  0:00:29s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 37 | loss: 0.00078 | val_0_mse: 0.00013 |  0:00:30s\n",
      "epoch 38 | loss: 0.00075 | val_0_mse: 0.00013 |  0:00:31s\n",
      "epoch 39 | loss: 0.00075 | val_0_mse: 0.00013 |  0:00:31s\n",
      "epoch 40 | loss: 0.00071 | val_0_mse: 0.00014 |  0:00:32s\n",
      "epoch 41 | loss: 0.00067 | val_0_mse: 0.00014 |  0:00:33s\n",
      "epoch 42 | loss: 0.00064 | val_0_mse: 0.00014 |  0:00:34s\n",
      "epoch 43 | loss: 0.00063 | val_0_mse: 0.00014 |  0:00:35s\n",
      "epoch 44 | loss: 0.00061 | val_0_mse: 0.00013 |  0:00:36s\n",
      "epoch 45 | loss: 0.00058 | val_0_mse: 0.00015 |  0:00:37s\n",
      "epoch 46 | loss: 0.00057 | val_0_mse: 0.00013 |  0:00:38s\n",
      "epoch 47 | loss: 0.00056 | val_0_mse: 0.00013 |  0:00:39s\n",
      "epoch 48 | loss: 0.00055 | val_0_mse: 0.00013 |  0:00:40s\n",
      "epoch 49 | loss: 0.00054 | val_0_mse: 0.00013 |  0:00:41s\n",
      "epoch 50 | loss: 0.00055 | val_0_mse: 0.00013 |  0:00:42s\n",
      "epoch 51 | loss: 0.00054 | val_0_mse: 0.00013 |  0:00:43s\n",
      "epoch 52 | loss: 0.00051 | val_0_mse: 0.00013 |  0:00:43s\n",
      "epoch 53 | loss: 0.0005  | val_0_mse: 0.00013 |  0:00:44s\n",
      "epoch 54 | loss: 0.00049 | val_0_mse: 0.00013 |  0:00:45s\n",
      "epoch 55 | loss: 0.00048 | val_0_mse: 0.00013 |  0:00:46s\n",
      "epoch 56 | loss: 0.00047 | val_0_mse: 0.00014 |  0:00:47s\n",
      "epoch 57 | loss: 0.00047 | val_0_mse: 0.00013 |  0:00:48s\n",
      "epoch 58 | loss: 0.00047 | val_0_mse: 0.00014 |  0:00:49s\n",
      "epoch 59 | loss: 0.00047 | val_0_mse: 0.00014 |  0:00:50s\n",
      "epoch 60 | loss: 0.00049 | val_0_mse: 0.00015 |  0:00:51s\n",
      "epoch 61 | loss: 0.00047 | val_0_mse: 0.00014 |  0:00:52s\n",
      "epoch 62 | loss: 0.00046 | val_0_mse: 0.00016 |  0:00:53s\n",
      "epoch 63 | loss: 0.00044 | val_0_mse: 0.00013 |  0:00:54s\n",
      "epoch 64 | loss: 0.00044 | val_0_mse: 0.00015 |  0:00:54s\n",
      "epoch 65 | loss: 0.00043 | val_0_mse: 0.00013 |  0:00:55s\n",
      "epoch 66 | loss: 0.00043 | val_0_mse: 0.00014 |  0:00:56s\n",
      "epoch 67 | loss: 0.00042 | val_0_mse: 0.00013 |  0:00:57s\n",
      "epoch 68 | loss: 0.00041 | val_0_mse: 0.00013 |  0:00:58s\n",
      "epoch 69 | loss: 0.00042 | val_0_mse: 0.00013 |  0:00:59s\n",
      "epoch 70 | loss: 0.00041 | val_0_mse: 0.00015 |  0:01:00s\n",
      "epoch 71 | loss: 0.00041 | val_0_mse: 0.00013 |  0:01:01s\n",
      "epoch 72 | loss: 0.00041 | val_0_mse: 0.00014 |  0:01:02s\n",
      "epoch 73 | loss: 0.0004  | val_0_mse: 0.00013 |  0:01:02s\n",
      "epoch 74 | loss: 0.0004  | val_0_mse: 0.00013 |  0:01:03s\n",
      "epoch 75 | loss: 0.00039 | val_0_mse: 0.00013 |  0:01:04s\n",
      "epoch 76 | loss: 0.00039 | val_0_mse: 0.00013 |  0:01:05s\n",
      "epoch 77 | loss: 0.00038 | val_0_mse: 0.00013 |  0:01:06s\n",
      "epoch 78 | loss: 0.00038 | val_0_mse: 0.00013 |  0:01:07s\n",
      "epoch 79 | loss: 0.00038 | val_0_mse: 0.00015 |  0:01:08s\n",
      "epoch 80 | loss: 0.00038 | val_0_mse: 0.00013 |  0:01:09s\n",
      "epoch 81 | loss: 0.00037 | val_0_mse: 0.00013 |  0:01:10s\n",
      "epoch 82 | loss: 0.00038 | val_0_mse: 0.00013 |  0:01:11s\n",
      "epoch 83 | loss: 0.00037 | val_0_mse: 0.00013 |  0:01:12s\n",
      "epoch 84 | loss: 0.00037 | val_0_mse: 0.00013 |  0:01:12s\n",
      "epoch 85 | loss: 0.00038 | val_0_mse: 0.00013 |  0:01:13s\n",
      "epoch 86 | loss: 0.00036 | val_0_mse: 0.00014 |  0:01:14s\n",
      "epoch 87 | loss: 0.00036 | val_0_mse: 0.00014 |  0:01:15s\n",
      "epoch 88 | loss: 0.00036 | val_0_mse: 0.00014 |  0:01:16s\n",
      "epoch 89 | loss: 0.00035 | val_0_mse: 0.00014 |  0:01:17s\n",
      "epoch 90 | loss: 0.00035 | val_0_mse: 0.00013 |  0:01:17s\n",
      "epoch 91 | loss: 0.00034 | val_0_mse: 0.00013 |  0:01:18s\n",
      "epoch 92 | loss: 0.00035 | val_0_mse: 0.00014 |  0:01:19s\n",
      "epoch 93 | loss: 0.00035 | val_0_mse: 0.00013 |  0:01:20s\n",
      "epoch 94 | loss: 0.00035 | val_0_mse: 0.00013 |  0:01:21s\n",
      "epoch 95 | loss: 0.00035 | val_0_mse: 0.00014 |  0:01:22s\n",
      "epoch 96 | loss: 0.00034 | val_0_mse: 0.00013 |  0:01:23s\n",
      "epoch 97 | loss: 0.00034 | val_0_mse: 0.00014 |  0:01:24s\n",
      "epoch 98 | loss: 0.00035 | val_0_mse: 0.00013 |  0:01:25s\n",
      "epoch 99 | loss: 0.00034 | val_0_mse: 0.00013 |  0:01:25s\n",
      "epoch 100| loss: 0.00033 | val_0_mse: 0.00014 |  0:01:26s\n",
      "epoch 101| loss: 0.00034 | val_0_mse: 0.00013 |  0:01:27s\n",
      "epoch 102| loss: 0.00036 | val_0_mse: 0.00014 |  0:01:28s\n",
      "epoch 103| loss: 0.00034 | val_0_mse: 0.00014 |  0:01:29s\n",
      "epoch 104| loss: 0.00033 | val_0_mse: 0.00013 |  0:01:30s\n",
      "epoch 105| loss: 0.00032 | val_0_mse: 0.00013 |  0:01:30s\n",
      "epoch 106| loss: 0.00032 | val_0_mse: 0.00014 |  0:01:31s\n",
      "epoch 107| loss: 0.00033 | val_0_mse: 0.00013 |  0:01:32s\n",
      "epoch 108| loss: 0.00033 | val_0_mse: 0.00014 |  0:01:33s\n",
      "epoch 109| loss: 0.00032 | val_0_mse: 0.00013 |  0:01:34s\n",
      "epoch 110| loss: 0.00032 | val_0_mse: 0.00013 |  0:01:35s\n",
      "epoch 111| loss: 0.00032 | val_0_mse: 0.00013 |  0:01:36s\n",
      "epoch 112| loss: 0.00032 | val_0_mse: 0.00014 |  0:01:37s\n",
      "epoch 113| loss: 0.00033 | val_0_mse: 0.00015 |  0:01:37s\n",
      "epoch 114| loss: 0.00033 | val_0_mse: 0.00013 |  0:01:38s\n",
      "epoch 115| loss: 0.00031 | val_0_mse: 0.00015 |  0:01:39s\n",
      "epoch 116| loss: 0.00033 | val_0_mse: 0.00013 |  0:01:40s\n",
      "epoch 117| loss: 0.00031 | val_0_mse: 0.00013 |  0:01:41s\n",
      "epoch 118| loss: 0.00032 | val_0_mse: 0.00013 |  0:01:42s\n",
      "epoch 119| loss: 0.00031 | val_0_mse: 0.00013 |  0:01:43s\n",
      "epoch 120| loss: 0.00031 | val_0_mse: 0.00013 |  0:01:44s\n",
      "epoch 121| loss: 0.0003  | val_0_mse: 0.00017 |  0:01:45s\n",
      "epoch 122| loss: 0.00034 | val_0_mse: 0.00017 |  0:01:46s\n",
      "epoch 123| loss: 0.00031 | val_0_mse: 0.00015 |  0:01:46s\n",
      "epoch 124| loss: 0.0003  | val_0_mse: 0.00014 |  0:01:47s\n",
      "epoch 125| loss: 0.0003  | val_0_mse: 0.00014 |  0:01:48s\n",
      "epoch 126| loss: 0.00029 | val_0_mse: 0.00014 |  0:01:49s\n",
      "epoch 127| loss: 0.0003  | val_0_mse: 0.00016 |  0:01:50s\n",
      "epoch 128| loss: 0.0003  | val_0_mse: 0.00021 |  0:01:51s\n",
      "epoch 129| loss: 0.00037 | val_0_mse: 0.00017 |  0:01:51s\n",
      "epoch 130| loss: 0.00033 | val_0_mse: 0.00015 |  0:01:52s\n",
      "epoch 131| loss: 0.00034 | val_0_mse: 0.00015 |  0:01:53s\n",
      "epoch 132| loss: 0.00033 | val_0_mse: 0.00015 |  0:01:54s\n",
      "epoch 133| loss: 0.00033 | val_0_mse: 0.00014 |  0:01:55s\n",
      "epoch 134| loss: 0.00035 | val_0_mse: 0.00014 |  0:01:56s\n",
      "epoch 135| loss: 0.00032 | val_0_mse: 0.00014 |  0:01:57s\n",
      "epoch 136| loss: 0.00033 | val_0_mse: 0.00014 |  0:01:57s\n",
      "epoch 137| loss: 0.00032 | val_0_mse: 0.00013 |  0:01:58s\n",
      "epoch 138| loss: 0.00032 | val_0_mse: 0.00013 |  0:01:59s\n",
      "epoch 139| loss: 0.00031 | val_0_mse: 0.00013 |  0:02:00s\n",
      "epoch 140| loss: 0.00031 | val_0_mse: 0.00013 |  0:02:01s\n",
      "epoch 141| loss: 0.00031 | val_0_mse: 0.00013 |  0:02:02s\n",
      "epoch 142| loss: 0.00031 | val_0_mse: 0.00013 |  0:02:03s\n",
      "epoch 143| loss: 0.00032 | val_0_mse: 0.00013 |  0:02:04s\n",
      "epoch 144| loss: 0.00031 | val_0_mse: 0.00013 |  0:02:04s\n",
      "epoch 145| loss: 0.0003  | val_0_mse: 0.00013 |  0:02:05s\n",
      "epoch 146| loss: 0.0003  | val_0_mse: 0.00013 |  0:02:06s\n",
      "epoch 147| loss: 0.0003  | val_0_mse: 0.00014 |  0:02:07s\n",
      "epoch 148| loss: 0.0003  | val_0_mse: 0.00014 |  0:02:08s\n",
      "epoch 149| loss: 0.0003  | val_0_mse: 0.00013 |  0:02:09s\n",
      "epoch 150| loss: 0.0003  | val_0_mse: 0.00013 |  0:02:09s\n",
      "epoch 151| loss: 0.00029 | val_0_mse: 0.00013 |  0:02:10s\n",
      "epoch 152| loss: 0.00029 | val_0_mse: 0.00013 |  0:02:11s\n",
      "epoch 153| loss: 0.00029 | val_0_mse: 0.00013 |  0:02:12s\n",
      "epoch 154| loss: 0.0003  | val_0_mse: 0.00013 |  0:02:13s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m df_file \u001b[38;5;129;01min\u001b[39;00m df_files:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(exp_times):\n\u001b[0;32m----> 3\u001b[0m         \u001b[43mexperiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36mexperiment\u001b[0;34m(df_file, N)\u001b[0m\n\u001b[1;32m     95\u001b[0m Y_test \u001b[38;5;241m=\u001b[39m Y_test\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     97\u001b[0m reg_low \u001b[38;5;241m=\u001b[39m TabNetRegressor(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtabnet_params) \n\u001b[0;32m---> 99\u001b[0m \u001b[43mreg_low\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m            \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m)\u001b[49m  \n\u001b[1;32m    104\u001b[0m Y_pred \u001b[38;5;241m=\u001b[39m reg_low\u001b[38;5;241m.\u001b[39mpredict(X_test)\u001b[38;5;241m.\u001b[39mflatten() \n\u001b[1;32m    105\u001b[0m exp \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexp_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-low\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:223\u001b[0m, in \u001b[0;36mTabModel.fit\u001b[0;34m(self, X_train, y_train, eval_set, eval_name, eval_metric, loss_fn, weights, max_epochs, patience, batch_size, virtual_batch_size, num_workers, drop_last, callbacks, pin_memory, from_unsupervised)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_epochs):\n\u001b[1;32m    219\u001b[0m \n\u001b[1;32m    220\u001b[0m     \u001b[38;5;66;03m# Call method on_epoch_begin for all callbacks\u001b[39;00m\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback_container\u001b[38;5;241m.\u001b[39mon_epoch_begin(epoch_idx)\n\u001b[0;32m--> 223\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;66;03m# Apply predict epoch to all eval sets\u001b[39;00m\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m eval_name, valid_dataloader \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(eval_names, valid_dataloaders):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:434\u001b[0m, in \u001b[0;36mTabModel._train_epoch\u001b[0;34m(self, train_loader)\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (X, y) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m    432\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback_container\u001b[38;5;241m.\u001b[39mon_batch_begin(batch_idx)\n\u001b[0;32m--> 434\u001b[0m     batch_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback_container\u001b[38;5;241m.\u001b[39mon_batch_end(batch_idx, batch_logs)\n\u001b[1;32m    438\u001b[0m epoch_logs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer\u001b[38;5;241m.\u001b[39mparam_groups[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:476\u001b[0m, in \u001b[0;36mTabModel._train_batch\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    473\u001b[0m loss \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlambda_sparse \u001b[38;5;241m*\u001b[39m M_loss\n\u001b[1;32m    475\u001b[0m \u001b[38;5;66;03m# Perform backward pass and optimization\u001b[39;00m\n\u001b[0;32m--> 476\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclip_value:\n\u001b[1;32m    478\u001b[0m     clip_grad_norm_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnetwork\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclip_value)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/tensor.py:221\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Tensor \u001b[38;5;129;01mand\u001b[39;00m has_torch_function(relevant_args):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    215\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    216\u001b[0m         relevant_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    219\u001b[0m         retain_graph\u001b[38;5;241m=\u001b[39mretain_graph,\n\u001b[1;32m    220\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph)\n\u001b[0;32m--> 221\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/autograd/__init__.py:130\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    128\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m--> 130\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for df_file in df_files:\n",
    "    for i in range(exp_times):\n",
    "        experiment(df_file, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(checkpoint_save_dir):\n",
    "    os.makedirs(checkpoint_save_dir, exist_ok=True)\n",
    "with open(saveas, 'w') as f:\n",
    "    for key in results.keys():\n",
    "        f.write(\"%s, %s\\n\" % (key, results[key]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
